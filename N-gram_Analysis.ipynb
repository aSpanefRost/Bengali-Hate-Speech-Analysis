{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "df=pd.read_csv(\"Bengali_hate_speech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>যত্তসব পাপন শালার ফাজলামী!!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>পাপন শালা রে রিমান্ডে নেওয়া দরকার</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>শালা লুচ্চা দেখতে পাঠার মত দেখা যায়</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate category\n",
       "0                     যত্তসব পাপন শালার ফাজলামী!!!!!     1   sports\n",
       "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার     1   sports\n",
       "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...     1   sports\n",
       "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়     1   sports\n",
       "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব     1   sports"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "    text = re.sub(punctuationNoPeriod, \"\", text)   \n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_strings(texts, replace):\n",
    "    new_texts=[]\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
    "    \n",
    "    for text in texts:\n",
    "        for r in replace:\n",
    "            text=text.replace(r[0], r[1])\n",
    "        text=emoji_pattern.sub(r'', text)\n",
    "        text=english_pattern.sub(r'', text)\n",
    "        text=re.sub(r'\\s+', ' ', text).strip()\n",
    "        new_texts.append(text)\n",
    "\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace=[('\\u200c', ' '),\n",
    "         ('\\u200d', ' '),\n",
    "        ('\\xa0', ' '),\n",
    "        ('\\n', ' '),\n",
    "        ('\\r', ' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = basic_clean(''.join(str(df['sentence'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = basic_clean(''.join(str(df['sentence'][df.hate==1].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Analysis in Hate Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigrams_series1 = (pd.Series(nltk.ngrams(words1, 1)).value_counts())[:20]\n",
    "Bigrams_series1 = (pd.Series(nltk.ngrams(words1, 2)).value_counts())[:20]\n",
    "Trigrams_series1 = (pd.Series(nltk.ngrams(words1, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(এই,)         1815\n",
       "(না,)         1549\n",
       "(কি,)         1205\n",
       "(আর,)         1148\n",
       "(করে,)        1104\n",
       "(তুই,)        1000\n",
       "(কে,)          912\n",
       "(বাচ্চা,)      903\n",
       "(একটা,)        885\n",
       "(তোর,)         857\n",
       "(মাগি,)        635\n",
       "(সব,)          626\n",
       "(কুত্তার,)     607\n",
       "(খানকির,)      581\n",
       "(তো,)          568\n",
       "(মাগির,)       539\n",
       "(শালা,)        528\n",
       "(কথা,)         513\n",
       "(যে,)          483\n",
       "(জুতা,)        481\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigrams_series1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(কুত্তার, বাচ্চা)    257\n",
       "(খানকির, পোলা)       203\n",
       "(মাগির, মাগির)       127\n",
       "(মনে, হয়)            124\n",
       "(মাদার, চোদ)         102\n",
       "(তুই, একটা)           79\n",
       "(খানকি, মাগি)         77\n",
       "(এই, সব)              76\n",
       "(জুতা, দিয়ে)          60\n",
       "(এই, কুত্তার)         59\n",
       "(কথা, বলে)            58\n",
       "(এই, খানকির)          58\n",
       "(মারে, চুদি)          57\n",
       "(লুচ্চা, লুচ্চা)      56\n",
       "(হা, হা)              54\n",
       "(জুতা, পিটা)          53\n",
       "(জুতা, মার)           53\n",
       "(কে, জুতা)            49\n",
       "(পারে, না)            49\n",
       "(মাসুদ, রানা)         49\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bigrams_series1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(মাগির, মাগির, মাগির)       126\n",
       "(লুচ্চা, লুচ্চা, লুচ্চা)     52\n",
       "(হা, হা, হা)                 24\n",
       "(চোর, চোর, চোর)              20\n",
       "(জুতা, পিটা, করা)            19\n",
       "(জুতা, মারা, দরকার)          18\n",
       "(জুতা, পেটা, করা)            18\n",
       "(ছি, ছি, ছি)                 17\n",
       "(এই, খানকির, পোলা)           16\n",
       "(থেকে, বের, করে)             16\n",
       "(এই, কুত্তার, বাচ্চা)        15\n",
       "(কুত্তার, বাচ্চা, তুই)       15\n",
       "(আমার, মনে, হয়)              15\n",
       "(তোর, মারে, চুদি)            13\n",
       "(করতে, পারে, না)             13\n",
       "(পাপন, খানকির, পোলা)         13\n",
       "(ছিঃ, ছিঃ, ছিঃ)              13\n",
       "(কুত্তার, বাচ্চা, কে)        12\n",
       "(মাগি, মাগি, মাগি)           12\n",
       "(খানকির, পোলা, তোর)          12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trigrams_series1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words0 = basic_clean(''.join(str(df['sentence'][df.hate==0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Analysis in Non-Hate Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigrams_series0 = (pd.Series(nltk.ngrams(words0, 1)).value_counts())[:20]\n",
    "Bigrams_series0 = (pd.Series(nltk.ngrams(words0, 2)).value_counts())[:20]\n",
    "Trigrams_series0 = (pd.Series(nltk.ngrams(words0, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(না,)       4199\n",
       "(করে,)      2915\n",
       "(এই,)       2869\n",
       "(আর,)       2696\n",
       "(কি,)       2597\n",
       "(কে,)       1876\n",
       "(আমি,)      1616\n",
       "(আমার,)     1603\n",
       "(আপনার,)    1514\n",
       "(কথা,)      1504\n",
       "(ভাই,)      1423\n",
       "(ও,)        1378\n",
       "(জন্য,)     1347\n",
       "(যে,)       1344\n",
       "(একটা,)     1327\n",
       "(ভালো,)     1280\n",
       "(আপনি,)     1278\n",
       "(অনেক,)     1232\n",
       "(তার,)      1219\n",
       "(থেকে,)     1166\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigrams_series0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(মনে, হয়)        233\n",
       "(হা, হা)         167\n",
       "(অনেক, ভালো)     126\n",
       "(করার, জন্য)     125\n",
       "(খুব, ভালো)      119\n",
       "(জাফর, ইকবাল)    117\n",
       "(কথা, বলে)       115\n",
       "(হবে, না)        113\n",
       "(এই, সব)         112\n",
       "(না, করে)        112\n",
       "(আমার, মনে)      104\n",
       "(ভালো, লাগে)     100\n",
       "(না, কেন)         94\n",
       "(হয়, না)          93\n",
       "(এই, রকম)         91\n",
       "(হতে, পারে)       85\n",
       "(না, হলে)         84\n",
       "(কি, করে)         82\n",
       "(করা, হোক)        82\n",
       "(মনে, হচ্ছে)      80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bigrams_series0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(হা, হা, হা)                76\n",
       "(আমার, মনে, হয়)             61\n",
       "(না, না, না)                52\n",
       "(আমি, মনে, করি)             49\n",
       "(করে, বমি, করে)             29\n",
       "(জাফর, ইকবাল, স্যার)        28\n",
       "(অনেক, অনেক, ধন্যবাদ)       27\n",
       "(বি, এন, পি)                25\n",
       "(সাকিব, আল, হাসান)          24\n",
       "(খুব, ভালো, লাগলো)          24\n",
       "(মিজানুর, রহমান, আজহারী)    22\n",
       "(থেকে, বের, করে)            20\n",
       "(হড়হড়, করে, বমি)            20\n",
       "(অনেক, দিন, পর)             18\n",
       "(ছি, ছি, ছি)                18\n",
       "(তা, না, হলে)               18\n",
       "(যদি, পারেন, তাহলে)         17\n",
       "(হড়, হড়, করে)               17\n",
       "(অনেক, ভালো, লাগলো)         17\n",
       "(এই, ভিডিও, টা)             16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trigrams_series0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Analysis in Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigrams_series = (pd.Series(nltk.ngrams(words, 1)).value_counts())[:20]\n",
    "Bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]\n",
    "Trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(না,)       5748\n",
       "(এই,)       4684\n",
       "(করে,)      4019\n",
       "(আর,)       3844\n",
       "(কি,)       3802\n",
       "(কে,)       2788\n",
       "(একটা,)     2212\n",
       "(আমি,)      2061\n",
       "(কথা,)      2017\n",
       "(আমার,)     1999\n",
       "(ও,)        1849\n",
       "(যে,)       1827\n",
       "(জন্য,)     1729\n",
       "(তো,)       1713\n",
       "(ভাই,)      1688\n",
       "(সব,)       1684\n",
       "(আপনার,)    1642\n",
       "(এর,)       1580\n",
       "(ভালো,)     1573\n",
       "(তার,)      1562\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigrams_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(মনে, হয়)            357\n",
       "(কুত্তার, বাচ্চা)    265\n",
       "(হা, হা)             221\n",
       "(খানকির, পোলা)       206\n",
       "(এই, সব)             188\n",
       "(কথা, বলে)           173\n",
       "(করার, জন্য)         161\n",
       "(জাফর, ইকবাল)        158\n",
       "(না, করে)            151\n",
       "(হবে, না)            148\n",
       "(অনেক, ভালো)         140\n",
       "(আমার, মনে)          132\n",
       "(খুব, ভালো)          132\n",
       "(না, কেন)            132\n",
       "(করা, হোক)           129\n",
       "(এই, রকম)            127\n",
       "(মাগির, মাগির)       127\n",
       "(কি, করে)            126\n",
       "(না, হলে)            122\n",
       "(ছি, ছি)             118\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bigrams_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(মাগির, মাগির, মাগির)       126\n",
       "(হা, হা, হা)                100\n",
       "(আমার, মনে, হয়)              76\n",
       "(আমি, মনে, করি)              60\n",
       "(না, না, না)                 52\n",
       "(লুচ্চা, লুচ্চা, লুচ্চা)     52\n",
       "(থেকে, বের, করে)             36\n",
       "(ছি, ছি, ছি)                 35\n",
       "(করে, বমি, করে)              33\n",
       "(বি, এন, পি)                 32\n",
       "(জাফর, ইকবাল, স্যার)         31\n",
       "(অনেক, অনেক, ধন্যবাদ)        31\n",
       "(জুতা, পিটা, করা)            31\n",
       "(জুতা, পেটা, করা)            30\n",
       "(ছিঃ, ছিঃ, ছিঃ)              29\n",
       "(তা, না, হলে)                29\n",
       "(জুতা, মারা, দরকার)          29\n",
       "(সাকিব, আল, হাসান)           28\n",
       "(করতে, পারে, না)             25\n",
       "(খুব, ভালো, লাগলো)           25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trigrams_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438630"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word2Vec model with vector size of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec([words], size=300, window=50, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        যত্তসব পাপন শালার ফাজলামী!!!!!\n",
       "1                     পাপন শালা রে রিমান্ডে নেওয়া দরকার\n",
       "2     জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...\n",
       "3                   শালা লুচ্চা দেখতে পাঠার মত দেখা যায়\n",
       "4      তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব\n",
       "5     এটা কুন দরনের কেলা ফাইজলামি তাস্কিন রে চর মারা...\n",
       "6                      পাপন ভর মাদা চোদ পাপনে পদতেক চাই\n",
       "7                                 দুরো সালার পুদ চুপথাক\n",
       "8                                    কুত্তার বাছচা পাপন\n",
       "9                                     বাল ছাল তর সাউয়া😡\n",
       "10                        তোর কপালে জুতা মারি শালার পুত\n",
       "11                                     পাপনে পাগল হয়াছে\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['যত্তসব', 'পাপন', 'শালার', 'ফাজলামী', 'পাপন', 'শালা', 'রে', 'রিমান্ডে', 'নেওয়া', 'দরকার']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('কিছুর', 0.4087870419025421),\n",
       " ('কাপ', 0.4079126715660095),\n",
       " ('ইউটিউবার', 0.40174606442451477),\n",
       " ('নবীকে', 0.40031445026397705),\n",
       " ('কারণে', 0.39885151386260986)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('বাংগালী', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"embedding_word2vec.txt\"\n",
    "model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>যত্তসব পাপন শালার ফাজলামী!!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>পাপন শালা রে রিমান্ডে নেওয়া দরকার</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>শালা লুচ্চা দেখতে পাঠার মত দেখা যায়</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate category\n",
       "0                     যত্তসব পাপন শালার ফাজলামী!!!!!     1   sports\n",
       "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার     1   sports\n",
       "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...     1   sports\n",
       "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়     1   sports\n",
       "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব     1   sports"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.63137862e-02, -1.41983449e-01,  1.46264432e-03, -1.83959365e-01,\n",
       "       -1.69953704e-02, -1.87491495e-02, -1.97759978e-02,  9.70379859e-02,\n",
       "       -6.46867454e-02, -4.69931550e-02, -2.71046218e-02, -2.91423261e-01,\n",
       "        5.63621754e-03,  2.72657692e-01,  2.25436792e-01, -3.12920772e-02,\n",
       "        9.59689468e-02, -1.02639673e-02,  1.15689799e-01, -3.07544097e-02,\n",
       "        6.43213987e-02,  7.57659376e-02, -2.50332803e-01, -1.21727422e-01,\n",
       "        6.48555979e-02, -1.42652646e-01, -9.68732387e-02,  1.59352154e-01,\n",
       "       -7.25110695e-02, -1.54783586e-02,  3.88745288e-03, -8.50437507e-02,\n",
       "       -2.86644530e-02, -1.07964240e-01,  2.21488893e-01,  3.41851622e-01,\n",
       "        5.34537900e-03, -1.41455308e-01,  8.24988820e-03,  7.60176852e-02,\n",
       "       -8.42386708e-02,  2.81526726e-02,  1.66850984e-01,  9.96643901e-02,\n",
       "        1.09456867e-01,  1.61893010e-01, -1.36011109e-01,  7.08211511e-02,\n",
       "        1.24367848e-01, -6.55232221e-02, -2.29826927e-01,  3.38323154e-02,\n",
       "       -1.00689441e-01, -5.81296124e-02, -1.97037566e-03,  8.83799121e-02,\n",
       "       -1.25891939e-01,  6.25818446e-02,  2.42247060e-02,  5.69814108e-02,\n",
       "        4.44983058e-02,  5.65526746e-02, -7.66360909e-02,  3.24119210e-01,\n",
       "       -1.89094227e-02, -1.16617911e-01, -1.37778983e-01, -7.51966015e-02,\n",
       "        1.20315410e-01,  1.85727388e-01,  1.65288985e-01,  6.94444105e-02,\n",
       "        2.45830610e-01,  1.33283615e-01,  2.47803494e-01, -1.94336787e-01,\n",
       "       -4.68812277e-06, -2.21836522e-01, -1.34216666e-01, -1.40146121e-01,\n",
       "       -7.90167972e-02,  1.14828534e-01, -5.94612621e-02, -2.02398926e-01,\n",
       "        1.88765507e-02, -1.03108011e-01, -1.14403859e-01, -1.29007652e-01,\n",
       "        1.59099385e-01,  2.12665737e-01, -1.71418443e-01,  3.12673539e-01,\n",
       "       -1.62191585e-01,  2.80715734e-01, -1.12860158e-01,  5.40981218e-02,\n",
       "       -6.76199645e-02,  1.10357245e-02,  2.60794181e-02, -3.76190133e-02,\n",
       "       -1.84525013e-01, -1.04250908e-01,  1.09945061e-02,  9.81845409e-02,\n",
       "       -3.05379983e-02,  4.55637835e-02, -1.86332211e-01,  8.05477798e-02,\n",
       "        1.61903635e-01,  3.68890017e-02,  1.03773043e-01,  1.25927553e-01,\n",
       "        9.85662937e-02,  1.51134863e-01, -9.96915810e-03, -5.96055575e-02,\n",
       "       -1.66128650e-01, -1.01193801e-01, -5.69000281e-02,  3.92935611e-02,\n",
       "        4.59817573e-02,  1.04854837e-01,  2.10691214e-01, -2.08530217e-01,\n",
       "       -9.81898047e-03,  1.60747722e-01, -9.92925465e-02, -1.30456001e-01,\n",
       "        9.53517631e-02, -2.59499222e-01, -2.46856481e-01, -4.01902869e-02,\n",
       "        3.31109948e-02, -1.98901240e-02,  1.63936540e-01,  1.01446204e-01,\n",
       "       -1.37912016e-03,  4.53557745e-02, -2.83800364e-01, -2.18329821e-02,\n",
       "        1.86270103e-01,  5.03433309e-02,  4.26857322e-02,  1.59589455e-01,\n",
       "       -8.19404647e-02,  1.14256199e-02, -3.27975079e-02,  9.89250392e-02,\n",
       "       -5.05217649e-02, -2.43431311e-02,  3.22234109e-02, -1.79705501e-01,\n",
       "        4.04546596e-02, -2.74370983e-02,  1.30440056e-01, -4.16451767e-02,\n",
       "        5.98319322e-02,  1.58404484e-01,  1.85173042e-02,  2.46863246e-01,\n",
       "        2.09874120e-02,  1.04197048e-01, -2.10414439e-01,  3.10044438e-01,\n",
       "        1.06536351e-01,  8.01719204e-02, -6.07596375e-02, -3.02117821e-02,\n",
       "       -7.18099624e-03, -6.57087415e-02, -8.69844854e-02, -2.28019003e-02,\n",
       "        1.08077683e-01,  1.07553139e-01,  1.36891171e-01, -1.02810115e-01,\n",
       "       -1.50605395e-01, -2.42186174e-01, -2.39708088e-02,  2.58768529e-01,\n",
       "       -1.27944738e-01,  5.55343777e-02, -8.69635940e-02,  9.42910910e-02,\n",
       "        1.53106883e-01,  4.35665511e-02, -2.32576244e-02, -1.79951146e-01,\n",
       "        1.50781974e-01,  1.73125267e-01, -2.04866361e-02,  7.04127029e-02,\n",
       "       -9.71099958e-02, -2.26588291e-03,  4.98169735e-02, -2.76273578e-01,\n",
       "        6.77655963e-03,  3.12845856e-02, -3.17362957e-02,  2.79089540e-01,\n",
       "       -8.40485170e-02,  5.01157157e-03,  6.43394813e-02,  6.19956106e-02,\n",
       "       -1.22396097e-01, -1.96359772e-02, -1.97475366e-02, -5.56488186e-02,\n",
       "        2.17430256e-02,  2.22027618e-02,  1.76808193e-01,  1.37148485e-01,\n",
       "       -1.40353311e-02,  5.51469252e-02, -1.86037809e-01, -6.83106706e-02,\n",
       "        3.47891748e-02,  1.54977202e-01,  2.04176232e-01, -3.09110492e-01,\n",
       "        8.03493783e-02,  1.21607393e-01, -9.72506478e-02,  2.12526128e-01,\n",
       "       -1.99959472e-01,  2.16331959e-01,  1.08667068e-01, -7.28735700e-02,\n",
       "       -5.90941161e-02, -5.29236346e-02,  5.24707921e-02,  4.91455682e-02,\n",
       "        8.34446307e-03,  9.67930779e-02, -3.84202041e-02,  1.38501227e-01,\n",
       "       -1.25117987e-01, -2.68079322e-02,  4.30531614e-03, -6.24291562e-02,\n",
       "       -3.26063156e-01, -4.27797921e-02,  2.40591705e-01, -9.14649963e-02,\n",
       "       -1.11748546e-01, -1.49440452e-01, -2.27857335e-03, -1.44834686e-02,\n",
       "        2.17477068e-01,  4.07037660e-02,  3.35516363e-01,  1.37193412e-01,\n",
       "       -5.50371446e-02,  8.76790211e-02,  3.04473788e-01, -4.97924648e-02,\n",
       "       -4.25633714e-02, -2.24937350e-01,  1.68456987e-01, -1.74146101e-01,\n",
       "        9.16325077e-02, -1.29596400e-03,  1.71812333e-03, -1.45944059e-01,\n",
       "       -1.33771420e-01,  8.86583552e-02,  1.33844540e-01,  7.34431949e-03,\n",
       "       -7.70301446e-02, -2.45014176e-01, -1.15679078e-01,  1.06005762e-02,\n",
       "        6.74246699e-02,  9.38027352e-02,  8.11882615e-02, -6.07844964e-02,\n",
       "       -7.81628415e-02,  1.01528406e-01,  3.89128923e-03, -2.45812207e-01,\n",
       "        1.64964333e-01, -6.11242540e-02,  2.61933863e-01,  2.33187944e-01,\n",
       "       -1.92257568e-01,  3.69829834e-02,  8.47233236e-02, -5.11525273e-02,\n",
       "        2.51186937e-01, -8.17087293e-02, -3.47444415e-02,  8.67645070e-02,\n",
       "       -4.07194234e-02,  1.79043598e-02,  1.10184245e-01,  5.61492294e-02,\n",
       "        1.04992263e-01, -1.10398941e-01, -2.13208288e-01,  1.13480270e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['পাপন']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           যত্তসব পাপন শালার ফাজলামী!!!!!\n",
       "1                        পাপন শালা রে রিমান্ডে নেওয়া দরকার\n",
       "2        জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...\n",
       "3                      শালা লুচ্চা দেখতে পাঠার মত দেখা যায়\n",
       "4         তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব\n",
       "                               ...                        \n",
       "29995                        আমার মনে হচ্ছে মেনে নেয়া উচিত\n",
       "29996                         আমি ধন্যবাদ জানাই আইনপসাসনকে\n",
       "29997             কাসমির কাসমিরই নিজশ্যই সাদিন হওয়ার দরকার\n",
       "29998                   কলমি পিলিজ আপু মনি অনেক কিওট লাগছে\n",
       "29999                           আমি পাকিস্তান এর সাথে জড়িত\n",
       "Name: sentence, Length: 30000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors = pd.DataFrame()\n",
    "\n",
    "for doc in df['sentence']:\n",
    "    temp = pd.DataFrame()\n",
    "    for word_a in doc.split():\n",
    "        try:\n",
    "            word_vec=model.wv[word_a]\n",
    "            temp=temp.append(pd.Series(word_vec),ignore_index=True)  \n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean()\n",
    "    docs_vectors=docs_vectors.append(doc_vector,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors['hate'] = df['hate']\n",
    "docs_vectors = docs_vectors.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset in Test & Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23962, 300), (23962,), (5991, 300), (5991,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('hate', axis = 1),\n",
    "                                                   docs_vectors['hate'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression To Classify the Embaddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.694041\n",
      "Precision: 0.663430\n",
      "Recall: 0.201474\n",
      "F1 score: 0.309084\n",
      "ROC AUC: 0.574448\n"
     ]
    }
   ],
   "source": [
    "test_pred = logisticRegr.predict(test_x)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "accuracy=accuracy_score(test_y, test_pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(test_y, test_pred)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(test_y, test_pred)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(test_y, test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(test_y, test_pred)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM to classify the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.701052\n",
      "Precision: 0.659686\n",
      "Recall: 0.247666\n",
      "F1 score: 0.360129\n",
      "ROC AUC: 0.590971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC() # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "test_pred = clf.predict(test_x)\n",
    "accuracy=accuracy_score(test_y, test_pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(test_y, test_pred)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(test_y, test_pred)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(test_y, test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(test_y, test_pred)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTM Model To build the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index={}\n",
    "f=open(os.path.join('','embedding_word2vec.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    value=line.split()\n",
    "    word=value[0]\n",
    "    coefs=np.asarray(value[1:])\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOund 57362 unique tokens.\n",
      "shape of review tensor: (30000, 537)\n",
      "shape of sentiment tensor: (30000,)\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in df['sentence']])\n",
    "#max_length=537\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(df['sentence'])\n",
    "sequences = tokenizer_obj.texts_to_sequences(df['sentence'])\n",
    "\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('FOund %s unique tokens.'%len(word_index))\n",
    "\n",
    "review_pad=pad_sequences(sequences,maxlen=max_length)\n",
    "sentiment=df['hate'].values\n",
    "print('shape of review tensor:',review_pad.shape)\n",
    "print('shape of sentiment tensor:', sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in df['sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59062"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]= embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57363\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 537)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57363, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "modell=Sequential()\n",
    "embedding_layer = Embedding(num_words, 300, embeddings_initializer=Constant(embedding_matrix),input_length=max_length,trainable=False)\n",
    "modell.add(embedding_layer)\n",
    "modell.add(LSTM(units=32,dropout=0.2,recurrent_dropout=0.2))\n",
    "modell.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 537, 300)          17208900  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 17,251,557\n",
      "Trainable params: 42,657\n",
      "Non-trainable params: 17,208,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modell.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT=0.2\n",
    "indices=np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "sentiment=sentiment[indices]\n",
    "num_validation_samples=int(VALIDATION_SPLIT*review_pad.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainn_pad=review_pad[:-num_validation_samples]\n",
    "y_trainn=sentiment[:-num_validation_samples]\n",
    "x_testt_pad=review_pad[-num_validation_samples:]\n",
    "y_testt=sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 537)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_testt_pad.shape)\n",
    "print(y_testt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "188/188 [==============================] - 330s 2s/step - loss: 0.6525 - accuracy: 0.6592\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 291s 2s/step - loss: 0.6435 - accuracy: 0.6578\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 281s 1s/step - loss: 0.6358 - accuracy: 0.6682\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 323s 2s/step - loss: 0.6381 - accuracy: 0.6649\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 305s 2s/step - loss: 0.6349 - accuracy: 0.6695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fd6d933d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modell.fit(x_trainn_pad,y_trainn,batch_size=128,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = modell.predict(x_testt_pad)\n",
    "# accuracy=accuracy_score(y_testt, test_pred)\n",
    "# print('Accuracy: %f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_testt, test_pred)\n",
    "# print('Precision: %f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_testt, test_pred)\n",
    "# print('Recall: %f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_testt, test_pred)\n",
    "# print('F1 score: %f' % f1)\n",
    "# # ROC AUC\n",
    "# auc = roc_auc_score(y_testt, test_pred)\n",
    "# print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.671500\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_testt, test_pred.round())\n",
    "print('Accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "test_pred_bool = np.argmax(test_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_testt, test_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_testt, test_pred , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.509493\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_testt, test_pred)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
