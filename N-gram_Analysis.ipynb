{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "df=pd.read_csv(\"Bengali_hate_speech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨ ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ!!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶∞‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶ú‡¶ø‡¶≤‡ßç‡¶≤‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ú‡¶æ‡¶∞‡¶ú ‡¶π‡¶¨‡ßá ‡¶è‡¶ü‡¶æ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶∂‡¶æ‡¶≤‡¶æ ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶∞ ‡¶Æ‡¶§ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡¶§‡ßÅ‡¶á ‡¶§‡ßã ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶ó‡¶æ‡¶ú‡¶æ ‡¶ñ‡¶æ‡¶á‡¶õ‡¶ö‡•§‡¶§‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡ßü ‡¶ñ‡ßá‡¶≤‡¶¨‡ßá ‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate category\n",
       "0                     ‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨ ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ!!!!!     1   sports\n",
       "1                  ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶∞‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞     1   sports\n",
       "2  ‡¶ú‡¶ø‡¶≤‡ßç‡¶≤‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ú‡¶æ‡¶∞‡¶ú ‡¶π‡¶¨‡ßá ‡¶è‡¶ü‡¶æ...     1   sports\n",
       "3                ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶∞ ‡¶Æ‡¶§ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü     1   sports\n",
       "4   ‡¶§‡ßÅ‡¶á ‡¶§‡ßã ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶ó‡¶æ‡¶ú‡¶æ ‡¶ñ‡¶æ‡¶á‡¶õ‡¶ö‡•§‡¶§‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡ßü ‡¶ñ‡ßá‡¶≤‡¶¨‡ßá ‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨     1   sports"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "    text = re.sub(punctuationNoPeriod, \"\", text)   \n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_strings(texts, replace):\n",
    "    new_texts=[]\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
    "    \n",
    "    for text in texts:\n",
    "        for r in replace:\n",
    "            text=text.replace(r[0], r[1])\n",
    "        text=emoji_pattern.sub(r'', text)\n",
    "        text=english_pattern.sub(r'', text)\n",
    "        text=re.sub(r'\\s+', ' ', text).strip()\n",
    "        new_texts.append(text)\n",
    "\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace=[('\\u200c', ' '),\n",
    "         ('\\u200d', ' '),\n",
    "        ('\\xa0', ' '),\n",
    "        ('\\n', ' '),\n",
    "        ('\\r', ' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = basic_clean(''.join(str(df['sentence'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = basic_clean(''.join(str(df['sentence'][df.hate==1].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Analysis in Hate Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigrams_series1 = (pd.Series(nltk.ngrams(words1, 1)).value_counts())[:20]\n",
    "Bigrams_series1 = (pd.Series(nltk.ngrams(words1, 2)).value_counts())[:20]\n",
    "Trigrams_series1 = (pd.Series(nltk.ngrams(words1, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶è‡¶á,)         1815\n",
       "(‡¶®‡¶æ,)         1549\n",
       "(‡¶ï‡¶ø,)         1205\n",
       "(‡¶Ü‡¶∞,)         1148\n",
       "(‡¶ï‡¶∞‡ßá,)        1104\n",
       "(‡¶§‡ßÅ‡¶á,)        1000\n",
       "(‡¶ï‡ßá,)          912\n",
       "(‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ,)      903\n",
       "(‡¶è‡¶ï‡¶ü‡¶æ,)        885\n",
       "(‡¶§‡ßã‡¶∞,)         857\n",
       "(‡¶Æ‡¶æ‡¶ó‡¶ø,)        635\n",
       "(‡¶∏‡¶¨,)          626\n",
       "(‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞,)     607\n",
       "(‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞,)      581\n",
       "(‡¶§‡ßã,)          568\n",
       "(‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞,)       539\n",
       "(‡¶∂‡¶æ‡¶≤‡¶æ,)        528\n",
       "(‡¶ï‡¶•‡¶æ,)         513\n",
       "(‡¶Ø‡ßá,)          483\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ,)        481\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigrams_series1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞, ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ)    257\n",
       "(‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞, ‡¶™‡ßã‡¶≤‡¶æ)       203\n",
       "(‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞, ‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞)       127\n",
       "(‡¶Æ‡¶®‡ßá, ‡¶π‡ßü)            124\n",
       "(‡¶Æ‡¶æ‡¶¶‡¶æ‡¶∞, ‡¶ö‡ßã‡¶¶)         102\n",
       "(‡¶§‡ßÅ‡¶á, ‡¶è‡¶ï‡¶ü‡¶æ)           79\n",
       "(‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø, ‡¶Æ‡¶æ‡¶ó‡¶ø)         77\n",
       "(‡¶è‡¶á, ‡¶∏‡¶¨)              76\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶¶‡¶ø‡ßü‡ßá)          60\n",
       "(‡¶è‡¶á, ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞)         59\n",
       "(‡¶ï‡¶•‡¶æ, ‡¶¨‡¶≤‡ßá)            58\n",
       "(‡¶è‡¶á, ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞)          58\n",
       "(‡¶Æ‡¶æ‡¶∞‡ßá, ‡¶ö‡ßÅ‡¶¶‡¶ø)          57\n",
       "(‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ)      56\n",
       "(‡¶π‡¶æ, ‡¶π‡¶æ)              54\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶™‡¶ø‡¶ü‡¶æ)          53\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶Æ‡¶æ‡¶∞)           53\n",
       "(‡¶ï‡ßá, ‡¶ú‡ßÅ‡¶§‡¶æ)            49\n",
       "(‡¶™‡¶æ‡¶∞‡ßá, ‡¶®‡¶æ)            49\n",
       "(‡¶Æ‡¶æ‡¶∏‡ßÅ‡¶¶, ‡¶∞‡¶æ‡¶®‡¶æ)         49\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bigrams_series1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞, ‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞, ‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞)       126\n",
       "(‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ)     52\n",
       "(‡¶π‡¶æ, ‡¶π‡¶æ, ‡¶π‡¶æ)                 24\n",
       "(‡¶ö‡ßã‡¶∞, ‡¶ö‡ßã‡¶∞, ‡¶ö‡ßã‡¶∞)              20\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶™‡¶ø‡¶ü‡¶æ, ‡¶ï‡¶∞‡¶æ)            19\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶Æ‡¶æ‡¶∞‡¶æ, ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞)          18\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶™‡ßá‡¶ü‡¶æ, ‡¶ï‡¶∞‡¶æ)            18\n",
       "(‡¶õ‡¶ø, ‡¶õ‡¶ø, ‡¶õ‡¶ø)                 17\n",
       "(‡¶è‡¶á, ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞, ‡¶™‡ßã‡¶≤‡¶æ)           16\n",
       "(‡¶•‡ßá‡¶ï‡ßá, ‡¶¨‡ßá‡¶∞, ‡¶ï‡¶∞‡ßá)             16\n",
       "(‡¶è‡¶á, ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞, ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ)        15\n",
       "(‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞, ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶§‡ßÅ‡¶á)       15\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞, ‡¶Æ‡¶®‡ßá, ‡¶π‡ßü)              15\n",
       "(‡¶§‡ßã‡¶∞, ‡¶Æ‡¶æ‡¶∞‡ßá, ‡¶ö‡ßÅ‡¶¶‡¶ø)            13\n",
       "(‡¶ï‡¶∞‡¶§‡ßá, ‡¶™‡¶æ‡¶∞‡ßá, ‡¶®‡¶æ)             13\n",
       "(‡¶™‡¶æ‡¶™‡¶®, ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞, ‡¶™‡ßã‡¶≤‡¶æ)         13\n",
       "(‡¶õ‡¶ø‡¶É, ‡¶õ‡¶ø‡¶É, ‡¶õ‡¶ø‡¶É)              13\n",
       "(‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞, ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶ï‡ßá)        12\n",
       "(‡¶Æ‡¶æ‡¶ó‡¶ø, ‡¶Æ‡¶æ‡¶ó‡¶ø, ‡¶Æ‡¶æ‡¶ó‡¶ø)           12\n",
       "(‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞, ‡¶™‡ßã‡¶≤‡¶æ, ‡¶§‡ßã‡¶∞)          12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trigrams_series1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words0 = basic_clean(''.join(str(df['sentence'][df.hate==0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Analysis in Non-Hate Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigrams_series0 = (pd.Series(nltk.ngrams(words0, 1)).value_counts())[:20]\n",
    "Bigrams_series0 = (pd.Series(nltk.ngrams(words0, 2)).value_counts())[:20]\n",
    "Trigrams_series0 = (pd.Series(nltk.ngrams(words0, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶®‡¶æ,)       4199\n",
       "(‡¶ï‡¶∞‡ßá,)      2915\n",
       "(‡¶è‡¶á,)       2869\n",
       "(‡¶Ü‡¶∞,)       2696\n",
       "(‡¶ï‡¶ø,)       2597\n",
       "(‡¶ï‡ßá,)       1876\n",
       "(‡¶Ü‡¶Æ‡¶ø,)      1616\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞,)     1603\n",
       "(‡¶Ü‡¶™‡¶®‡¶æ‡¶∞,)    1514\n",
       "(‡¶ï‡¶•‡¶æ,)      1504\n",
       "(‡¶≠‡¶æ‡¶á,)      1423\n",
       "(‡¶ì,)        1378\n",
       "(‡¶ú‡¶®‡ßç‡¶Ø,)     1347\n",
       "(‡¶Ø‡ßá,)       1344\n",
       "(‡¶è‡¶ï‡¶ü‡¶æ,)     1327\n",
       "(‡¶≠‡¶æ‡¶≤‡ßã,)     1280\n",
       "(‡¶Ü‡¶™‡¶®‡¶ø,)     1278\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï,)     1232\n",
       "(‡¶§‡¶æ‡¶∞,)      1219\n",
       "(‡¶•‡ßá‡¶ï‡ßá,)     1166\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigrams_series0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶Æ‡¶®‡ßá, ‡¶π‡ßü)        233\n",
       "(‡¶π‡¶æ, ‡¶π‡¶æ)         167\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï, ‡¶≠‡¶æ‡¶≤‡ßã)     126\n",
       "(‡¶ï‡¶∞‡¶æ‡¶∞, ‡¶ú‡¶®‡ßç‡¶Ø)     125\n",
       "(‡¶ñ‡ßÅ‡¶¨, ‡¶≠‡¶æ‡¶≤‡ßã)      119\n",
       "(‡¶ú‡¶æ‡¶´‡¶∞, ‡¶á‡¶ï‡¶¨‡¶æ‡¶≤)    117\n",
       "(‡¶ï‡¶•‡¶æ, ‡¶¨‡¶≤‡ßá)       115\n",
       "(‡¶π‡¶¨‡ßá, ‡¶®‡¶æ)        113\n",
       "(‡¶è‡¶á, ‡¶∏‡¶¨)         112\n",
       "(‡¶®‡¶æ, ‡¶ï‡¶∞‡ßá)        112\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞, ‡¶Æ‡¶®‡ßá)      104\n",
       "(‡¶≠‡¶æ‡¶≤‡ßã, ‡¶≤‡¶æ‡¶ó‡ßá)     100\n",
       "(‡¶®‡¶æ, ‡¶ï‡ßá‡¶®)         94\n",
       "(‡¶π‡ßü, ‡¶®‡¶æ)          93\n",
       "(‡¶è‡¶á, ‡¶∞‡¶ï‡¶Æ)         91\n",
       "(‡¶π‡¶§‡ßá, ‡¶™‡¶æ‡¶∞‡ßá)       85\n",
       "(‡¶®‡¶æ, ‡¶π‡¶≤‡ßá)         84\n",
       "(‡¶ï‡¶ø, ‡¶ï‡¶∞‡ßá)         82\n",
       "(‡¶ï‡¶∞‡¶æ, ‡¶π‡ßã‡¶ï)        82\n",
       "(‡¶Æ‡¶®‡ßá, ‡¶π‡¶ö‡ßç‡¶õ‡ßá)      80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bigrams_series0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶π‡¶æ, ‡¶π‡¶æ, ‡¶π‡¶æ)                76\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞, ‡¶Æ‡¶®‡ßá, ‡¶π‡ßü)             61\n",
       "(‡¶®‡¶æ, ‡¶®‡¶æ, ‡¶®‡¶æ)                52\n",
       "(‡¶Ü‡¶Æ‡¶ø, ‡¶Æ‡¶®‡ßá, ‡¶ï‡¶∞‡¶ø)             49\n",
       "(‡¶ï‡¶∞‡ßá, ‡¶¨‡¶Æ‡¶ø, ‡¶ï‡¶∞‡ßá)             29\n",
       "(‡¶ú‡¶æ‡¶´‡¶∞, ‡¶á‡¶ï‡¶¨‡¶æ‡¶≤, ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞)        28\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï, ‡¶Ö‡¶®‡ßá‡¶ï, ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶)       27\n",
       "(‡¶¨‡¶ø, ‡¶è‡¶®, ‡¶™‡¶ø)                25\n",
       "(‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨, ‡¶Ü‡¶≤, ‡¶π‡¶æ‡¶∏‡¶æ‡¶®)          24\n",
       "(‡¶ñ‡ßÅ‡¶¨, ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶≤‡¶æ‡¶ó‡¶≤‡ßã)          24\n",
       "(‡¶Æ‡¶ø‡¶ú‡¶æ‡¶®‡ßÅ‡¶∞, ‡¶∞‡¶π‡¶Æ‡¶æ‡¶®, ‡¶Ü‡¶ú‡¶π‡¶æ‡¶∞‡ßÄ)    22\n",
       "(‡¶•‡ßá‡¶ï‡ßá, ‡¶¨‡ßá‡¶∞, ‡¶ï‡¶∞‡ßá)            20\n",
       "(‡¶π‡ßú‡¶π‡ßú, ‡¶ï‡¶∞‡ßá, ‡¶¨‡¶Æ‡¶ø)            20\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï, ‡¶¶‡¶ø‡¶®, ‡¶™‡¶∞)             18\n",
       "(‡¶õ‡¶ø, ‡¶õ‡¶ø, ‡¶õ‡¶ø)                18\n",
       "(‡¶§‡¶æ, ‡¶®‡¶æ, ‡¶π‡¶≤‡ßá)               18\n",
       "(‡¶Ø‡¶¶‡¶ø, ‡¶™‡¶æ‡¶∞‡ßá‡¶®, ‡¶§‡¶æ‡¶π‡¶≤‡ßá)         17\n",
       "(‡¶π‡ßú, ‡¶π‡ßú, ‡¶ï‡¶∞‡ßá)               17\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï, ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶≤‡¶æ‡¶ó‡¶≤‡ßã)         17\n",
       "(‡¶è‡¶á, ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì, ‡¶ü‡¶æ)             16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trigrams_series0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Analysis in Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigrams_series = (pd.Series(nltk.ngrams(words, 1)).value_counts())[:20]\n",
    "Bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]\n",
    "Trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶®‡¶æ,)       5748\n",
       "(‡¶è‡¶á,)       4684\n",
       "(‡¶ï‡¶∞‡ßá,)      4019\n",
       "(‡¶Ü‡¶∞,)       3844\n",
       "(‡¶ï‡¶ø,)       3802\n",
       "(‡¶ï‡ßá,)       2788\n",
       "(‡¶è‡¶ï‡¶ü‡¶æ,)     2212\n",
       "(‡¶Ü‡¶Æ‡¶ø,)      2061\n",
       "(‡¶ï‡¶•‡¶æ,)      2017\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞,)     1999\n",
       "(‡¶ì,)        1849\n",
       "(‡¶Ø‡ßá,)       1827\n",
       "(‡¶ú‡¶®‡ßç‡¶Ø,)     1729\n",
       "(‡¶§‡ßã,)       1713\n",
       "(‡¶≠‡¶æ‡¶á,)      1688\n",
       "(‡¶∏‡¶¨,)       1684\n",
       "(‡¶Ü‡¶™‡¶®‡¶æ‡¶∞,)    1642\n",
       "(‡¶è‡¶∞,)       1580\n",
       "(‡¶≠‡¶æ‡¶≤‡ßã,)     1573\n",
       "(‡¶§‡¶æ‡¶∞,)      1562\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Unigrams_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶Æ‡¶®‡ßá, ‡¶π‡ßü)            357\n",
       "(‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞, ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ)    265\n",
       "(‡¶π‡¶æ, ‡¶π‡¶æ)             221\n",
       "(‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞, ‡¶™‡ßã‡¶≤‡¶æ)       206\n",
       "(‡¶è‡¶á, ‡¶∏‡¶¨)             188\n",
       "(‡¶ï‡¶•‡¶æ, ‡¶¨‡¶≤‡ßá)           173\n",
       "(‡¶ï‡¶∞‡¶æ‡¶∞, ‡¶ú‡¶®‡ßç‡¶Ø)         161\n",
       "(‡¶ú‡¶æ‡¶´‡¶∞, ‡¶á‡¶ï‡¶¨‡¶æ‡¶≤)        158\n",
       "(‡¶®‡¶æ, ‡¶ï‡¶∞‡ßá)            151\n",
       "(‡¶π‡¶¨‡ßá, ‡¶®‡¶æ)            148\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï, ‡¶≠‡¶æ‡¶≤‡ßã)         140\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞, ‡¶Æ‡¶®‡ßá)          132\n",
       "(‡¶ñ‡ßÅ‡¶¨, ‡¶≠‡¶æ‡¶≤‡ßã)          132\n",
       "(‡¶®‡¶æ, ‡¶ï‡ßá‡¶®)            132\n",
       "(‡¶ï‡¶∞‡¶æ, ‡¶π‡ßã‡¶ï)           129\n",
       "(‡¶è‡¶á, ‡¶∞‡¶ï‡¶Æ)            127\n",
       "(‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞, ‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞)       127\n",
       "(‡¶ï‡¶ø, ‡¶ï‡¶∞‡ßá)            126\n",
       "(‡¶®‡¶æ, ‡¶π‡¶≤‡ßá)            122\n",
       "(‡¶õ‡¶ø, ‡¶õ‡¶ø)             118\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bigrams_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-20 Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞, ‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞, ‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞)       126\n",
       "(‡¶π‡¶æ, ‡¶π‡¶æ, ‡¶π‡¶æ)                100\n",
       "(‡¶Ü‡¶Æ‡¶æ‡¶∞, ‡¶Æ‡¶®‡ßá, ‡¶π‡ßü)              76\n",
       "(‡¶Ü‡¶Æ‡¶ø, ‡¶Æ‡¶®‡ßá, ‡¶ï‡¶∞‡¶ø)              60\n",
       "(‡¶®‡¶æ, ‡¶®‡¶æ, ‡¶®‡¶æ)                 52\n",
       "(‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ, ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ)     52\n",
       "(‡¶•‡ßá‡¶ï‡ßá, ‡¶¨‡ßá‡¶∞, ‡¶ï‡¶∞‡ßá)             36\n",
       "(‡¶õ‡¶ø, ‡¶õ‡¶ø, ‡¶õ‡¶ø)                 35\n",
       "(‡¶ï‡¶∞‡ßá, ‡¶¨‡¶Æ‡¶ø, ‡¶ï‡¶∞‡ßá)              33\n",
       "(‡¶¨‡¶ø, ‡¶è‡¶®, ‡¶™‡¶ø)                 32\n",
       "(‡¶ú‡¶æ‡¶´‡¶∞, ‡¶á‡¶ï‡¶¨‡¶æ‡¶≤, ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞)         31\n",
       "(‡¶Ö‡¶®‡ßá‡¶ï, ‡¶Ö‡¶®‡ßá‡¶ï, ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶)        31\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶™‡¶ø‡¶ü‡¶æ, ‡¶ï‡¶∞‡¶æ)            31\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶™‡ßá‡¶ü‡¶æ, ‡¶ï‡¶∞‡¶æ)            30\n",
       "(‡¶õ‡¶ø‡¶É, ‡¶õ‡¶ø‡¶É, ‡¶õ‡¶ø‡¶É)              29\n",
       "(‡¶§‡¶æ, ‡¶®‡¶æ, ‡¶π‡¶≤‡ßá)                29\n",
       "(‡¶ú‡ßÅ‡¶§‡¶æ, ‡¶Æ‡¶æ‡¶∞‡¶æ, ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞)          29\n",
       "(‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨, ‡¶Ü‡¶≤, ‡¶π‡¶æ‡¶∏‡¶æ‡¶®)           28\n",
       "(‡¶ï‡¶∞‡¶§‡ßá, ‡¶™‡¶æ‡¶∞‡ßá, ‡¶®‡¶æ)             25\n",
       "(‡¶ñ‡ßÅ‡¶¨, ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶≤‡¶æ‡¶ó‡¶≤‡ßã)           25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trigrams_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438630"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word2Vec model with vector size of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec([words], size=300, window=50, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        ‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨ ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ!!!!!\n",
       "1                     ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶∞‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞\n",
       "2     ‡¶ú‡¶ø‡¶≤‡ßç‡¶≤‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ú‡¶æ‡¶∞‡¶ú ‡¶π‡¶¨‡ßá ‡¶è‡¶ü‡¶æ...\n",
       "3                   ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶∞ ‡¶Æ‡¶§ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü\n",
       "4      ‡¶§‡ßÅ‡¶á ‡¶§‡ßã ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶ó‡¶æ‡¶ú‡¶æ ‡¶ñ‡¶æ‡¶á‡¶õ‡¶ö‡•§‡¶§‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡ßü ‡¶ñ‡ßá‡¶≤‡¶¨‡ßá ‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨\n",
       "5     ‡¶è‡¶ü‡¶æ ‡¶ï‡ßÅ‡¶® ‡¶¶‡¶∞‡¶®‡ßá‡¶∞ ‡¶ï‡ßá‡¶≤‡¶æ ‡¶´‡¶æ‡¶á‡¶ú‡¶≤‡¶æ‡¶Æ‡¶ø ‡¶§‡¶æ‡¶∏‡ßç‡¶ï‡¶ø‡¶® ‡¶∞‡ßá ‡¶ö‡¶∞ ‡¶Æ‡¶æ‡¶∞‡¶æ...\n",
       "6                      ‡¶™‡¶æ‡¶™‡¶® ‡¶≠‡¶∞ ‡¶Æ‡¶æ‡¶¶‡¶æ ‡¶ö‡ßã‡¶¶ ‡¶™‡¶æ‡¶™‡¶®‡ßá ‡¶™‡¶¶‡¶§‡ßá‡¶ï ‡¶ö‡¶æ‡¶á\n",
       "7                                 ‡¶¶‡ßÅ‡¶∞‡ßã ‡¶∏‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶™‡ßÅ‡¶¶ ‡¶ö‡ßÅ‡¶™‡¶•‡¶æ‡¶ï\n",
       "8                                    ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶õ‡¶ö‡¶æ ‡¶™‡¶æ‡¶™‡¶®\n",
       "9                                     ‡¶¨‡¶æ‡¶≤ ‡¶õ‡¶æ‡¶≤ ‡¶§‡¶∞ ‡¶∏‡¶æ‡¶â‡ßü‡¶æüò°\n",
       "10                        ‡¶§‡ßã‡¶∞ ‡¶ï‡¶™‡¶æ‡¶≤‡ßá ‡¶ú‡ßÅ‡¶§‡¶æ ‡¶Æ‡¶æ‡¶∞‡¶ø ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶™‡ßÅ‡¶§\n",
       "11                                     ‡¶™‡¶æ‡¶™‡¶®‡ßá ‡¶™‡¶æ‡¶ó‡¶≤ ‡¶π‡ßü‡¶æ‡¶õ‡ßá\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨', '‡¶™‡¶æ‡¶™‡¶®', '‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞', '‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ', '‡¶™‡¶æ‡¶™‡¶®', '‡¶∂‡¶æ‡¶≤‡¶æ', '‡¶∞‡ßá', '‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá', '‡¶®‡ßá‡¶ì‡ßü‡¶æ', '‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡¶ï‡¶ø‡¶õ‡ßÅ‡¶∞', 0.4087870419025421),\n",
       " ('‡¶ï‡¶æ‡¶™', 0.4079126715660095),\n",
       " ('‡¶á‡¶â‡¶ü‡¶ø‡¶â‡¶¨‡¶æ‡¶∞', 0.40174606442451477),\n",
       " ('‡¶®‡¶¨‡ßÄ‡¶ï‡ßá', 0.40031445026397705),\n",
       " ('‡¶ï‡¶æ‡¶∞‡¶£‡ßá', 0.39885151386260986)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('‡¶¨‡¶æ‡¶Ç‡¶ó‡¶æ‡¶≤‡ßÄ', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"embedding_word2vec.txt\"\n",
    "model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨ ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ!!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶∞‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶ú‡¶ø‡¶≤‡ßç‡¶≤‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ú‡¶æ‡¶∞‡¶ú ‡¶π‡¶¨‡ßá ‡¶è‡¶ü‡¶æ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶∂‡¶æ‡¶≤‡¶æ ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶∞ ‡¶Æ‡¶§ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡¶§‡ßÅ‡¶á ‡¶§‡ßã ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶ó‡¶æ‡¶ú‡¶æ ‡¶ñ‡¶æ‡¶á‡¶õ‡¶ö‡•§‡¶§‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡ßü ‡¶ñ‡ßá‡¶≤‡¶¨‡ßá ‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate category\n",
       "0                     ‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨ ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ!!!!!     1   sports\n",
       "1                  ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶∞‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞     1   sports\n",
       "2  ‡¶ú‡¶ø‡¶≤‡ßç‡¶≤‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ú‡¶æ‡¶∞‡¶ú ‡¶π‡¶¨‡ßá ‡¶è‡¶ü‡¶æ...     1   sports\n",
       "3                ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶∞ ‡¶Æ‡¶§ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü     1   sports\n",
       "4   ‡¶§‡ßÅ‡¶á ‡¶§‡ßã ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶ó‡¶æ‡¶ú‡¶æ ‡¶ñ‡¶æ‡¶á‡¶õ‡¶ö‡•§‡¶§‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡ßü ‡¶ñ‡ßá‡¶≤‡¶¨‡ßá ‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨     1   sports"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.63137862e-02, -1.41983449e-01,  1.46264432e-03, -1.83959365e-01,\n",
       "       -1.69953704e-02, -1.87491495e-02, -1.97759978e-02,  9.70379859e-02,\n",
       "       -6.46867454e-02, -4.69931550e-02, -2.71046218e-02, -2.91423261e-01,\n",
       "        5.63621754e-03,  2.72657692e-01,  2.25436792e-01, -3.12920772e-02,\n",
       "        9.59689468e-02, -1.02639673e-02,  1.15689799e-01, -3.07544097e-02,\n",
       "        6.43213987e-02,  7.57659376e-02, -2.50332803e-01, -1.21727422e-01,\n",
       "        6.48555979e-02, -1.42652646e-01, -9.68732387e-02,  1.59352154e-01,\n",
       "       -7.25110695e-02, -1.54783586e-02,  3.88745288e-03, -8.50437507e-02,\n",
       "       -2.86644530e-02, -1.07964240e-01,  2.21488893e-01,  3.41851622e-01,\n",
       "        5.34537900e-03, -1.41455308e-01,  8.24988820e-03,  7.60176852e-02,\n",
       "       -8.42386708e-02,  2.81526726e-02,  1.66850984e-01,  9.96643901e-02,\n",
       "        1.09456867e-01,  1.61893010e-01, -1.36011109e-01,  7.08211511e-02,\n",
       "        1.24367848e-01, -6.55232221e-02, -2.29826927e-01,  3.38323154e-02,\n",
       "       -1.00689441e-01, -5.81296124e-02, -1.97037566e-03,  8.83799121e-02,\n",
       "       -1.25891939e-01,  6.25818446e-02,  2.42247060e-02,  5.69814108e-02,\n",
       "        4.44983058e-02,  5.65526746e-02, -7.66360909e-02,  3.24119210e-01,\n",
       "       -1.89094227e-02, -1.16617911e-01, -1.37778983e-01, -7.51966015e-02,\n",
       "        1.20315410e-01,  1.85727388e-01,  1.65288985e-01,  6.94444105e-02,\n",
       "        2.45830610e-01,  1.33283615e-01,  2.47803494e-01, -1.94336787e-01,\n",
       "       -4.68812277e-06, -2.21836522e-01, -1.34216666e-01, -1.40146121e-01,\n",
       "       -7.90167972e-02,  1.14828534e-01, -5.94612621e-02, -2.02398926e-01,\n",
       "        1.88765507e-02, -1.03108011e-01, -1.14403859e-01, -1.29007652e-01,\n",
       "        1.59099385e-01,  2.12665737e-01, -1.71418443e-01,  3.12673539e-01,\n",
       "       -1.62191585e-01,  2.80715734e-01, -1.12860158e-01,  5.40981218e-02,\n",
       "       -6.76199645e-02,  1.10357245e-02,  2.60794181e-02, -3.76190133e-02,\n",
       "       -1.84525013e-01, -1.04250908e-01,  1.09945061e-02,  9.81845409e-02,\n",
       "       -3.05379983e-02,  4.55637835e-02, -1.86332211e-01,  8.05477798e-02,\n",
       "        1.61903635e-01,  3.68890017e-02,  1.03773043e-01,  1.25927553e-01,\n",
       "        9.85662937e-02,  1.51134863e-01, -9.96915810e-03, -5.96055575e-02,\n",
       "       -1.66128650e-01, -1.01193801e-01, -5.69000281e-02,  3.92935611e-02,\n",
       "        4.59817573e-02,  1.04854837e-01,  2.10691214e-01, -2.08530217e-01,\n",
       "       -9.81898047e-03,  1.60747722e-01, -9.92925465e-02, -1.30456001e-01,\n",
       "        9.53517631e-02, -2.59499222e-01, -2.46856481e-01, -4.01902869e-02,\n",
       "        3.31109948e-02, -1.98901240e-02,  1.63936540e-01,  1.01446204e-01,\n",
       "       -1.37912016e-03,  4.53557745e-02, -2.83800364e-01, -2.18329821e-02,\n",
       "        1.86270103e-01,  5.03433309e-02,  4.26857322e-02,  1.59589455e-01,\n",
       "       -8.19404647e-02,  1.14256199e-02, -3.27975079e-02,  9.89250392e-02,\n",
       "       -5.05217649e-02, -2.43431311e-02,  3.22234109e-02, -1.79705501e-01,\n",
       "        4.04546596e-02, -2.74370983e-02,  1.30440056e-01, -4.16451767e-02,\n",
       "        5.98319322e-02,  1.58404484e-01,  1.85173042e-02,  2.46863246e-01,\n",
       "        2.09874120e-02,  1.04197048e-01, -2.10414439e-01,  3.10044438e-01,\n",
       "        1.06536351e-01,  8.01719204e-02, -6.07596375e-02, -3.02117821e-02,\n",
       "       -7.18099624e-03, -6.57087415e-02, -8.69844854e-02, -2.28019003e-02,\n",
       "        1.08077683e-01,  1.07553139e-01,  1.36891171e-01, -1.02810115e-01,\n",
       "       -1.50605395e-01, -2.42186174e-01, -2.39708088e-02,  2.58768529e-01,\n",
       "       -1.27944738e-01,  5.55343777e-02, -8.69635940e-02,  9.42910910e-02,\n",
       "        1.53106883e-01,  4.35665511e-02, -2.32576244e-02, -1.79951146e-01,\n",
       "        1.50781974e-01,  1.73125267e-01, -2.04866361e-02,  7.04127029e-02,\n",
       "       -9.71099958e-02, -2.26588291e-03,  4.98169735e-02, -2.76273578e-01,\n",
       "        6.77655963e-03,  3.12845856e-02, -3.17362957e-02,  2.79089540e-01,\n",
       "       -8.40485170e-02,  5.01157157e-03,  6.43394813e-02,  6.19956106e-02,\n",
       "       -1.22396097e-01, -1.96359772e-02, -1.97475366e-02, -5.56488186e-02,\n",
       "        2.17430256e-02,  2.22027618e-02,  1.76808193e-01,  1.37148485e-01,\n",
       "       -1.40353311e-02,  5.51469252e-02, -1.86037809e-01, -6.83106706e-02,\n",
       "        3.47891748e-02,  1.54977202e-01,  2.04176232e-01, -3.09110492e-01,\n",
       "        8.03493783e-02,  1.21607393e-01, -9.72506478e-02,  2.12526128e-01,\n",
       "       -1.99959472e-01,  2.16331959e-01,  1.08667068e-01, -7.28735700e-02,\n",
       "       -5.90941161e-02, -5.29236346e-02,  5.24707921e-02,  4.91455682e-02,\n",
       "        8.34446307e-03,  9.67930779e-02, -3.84202041e-02,  1.38501227e-01,\n",
       "       -1.25117987e-01, -2.68079322e-02,  4.30531614e-03, -6.24291562e-02,\n",
       "       -3.26063156e-01, -4.27797921e-02,  2.40591705e-01, -9.14649963e-02,\n",
       "       -1.11748546e-01, -1.49440452e-01, -2.27857335e-03, -1.44834686e-02,\n",
       "        2.17477068e-01,  4.07037660e-02,  3.35516363e-01,  1.37193412e-01,\n",
       "       -5.50371446e-02,  8.76790211e-02,  3.04473788e-01, -4.97924648e-02,\n",
       "       -4.25633714e-02, -2.24937350e-01,  1.68456987e-01, -1.74146101e-01,\n",
       "        9.16325077e-02, -1.29596400e-03,  1.71812333e-03, -1.45944059e-01,\n",
       "       -1.33771420e-01,  8.86583552e-02,  1.33844540e-01,  7.34431949e-03,\n",
       "       -7.70301446e-02, -2.45014176e-01, -1.15679078e-01,  1.06005762e-02,\n",
       "        6.74246699e-02,  9.38027352e-02,  8.11882615e-02, -6.07844964e-02,\n",
       "       -7.81628415e-02,  1.01528406e-01,  3.89128923e-03, -2.45812207e-01,\n",
       "        1.64964333e-01, -6.11242540e-02,  2.61933863e-01,  2.33187944e-01,\n",
       "       -1.92257568e-01,  3.69829834e-02,  8.47233236e-02, -5.11525273e-02,\n",
       "        2.51186937e-01, -8.17087293e-02, -3.47444415e-02,  8.67645070e-02,\n",
       "       -4.07194234e-02,  1.79043598e-02,  1.10184245e-01,  5.61492294e-02,\n",
       "        1.04992263e-01, -1.10398941e-01, -2.13208288e-01,  1.13480270e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['‡¶™‡¶æ‡¶™‡¶®']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           ‡¶Ø‡¶§‡ßç‡¶§‡¶∏‡¶¨ ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶ú‡¶≤‡¶æ‡¶Æ‡ßÄ!!!!!\n",
       "1                        ‡¶™‡¶æ‡¶™‡¶® ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶∞‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞\n",
       "2        ‡¶ú‡¶ø‡¶≤‡ßç‡¶≤‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ú‡¶æ‡¶∞‡¶ú ‡¶π‡¶¨‡ßá ‡¶è‡¶ü‡¶æ...\n",
       "3                      ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶≤‡ßÅ‡¶ö‡ßç‡¶ö‡¶æ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶∞ ‡¶Æ‡¶§ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü\n",
       "4         ‡¶§‡ßÅ‡¶á ‡¶§‡ßã ‡¶∂‡¶æ‡¶≤‡¶æ ‡¶ó‡¶æ‡¶ú‡¶æ ‡¶ñ‡¶æ‡¶á‡¶õ‡¶ö‡•§‡¶§‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡ßü ‡¶ñ‡ßá‡¶≤‡¶¨‡ßá ‡¶∏‡¶æ‡¶ï‡¶ø‡¶¨\n",
       "                               ...                        \n",
       "29995                        ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶®‡ßá ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Æ‡ßá‡¶®‡ßá ‡¶®‡ßá‡ßü‡¶æ ‡¶â‡¶ö‡¶ø‡¶§\n",
       "29996                         ‡¶Ü‡¶Æ‡¶ø ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶ ‡¶ú‡¶æ‡¶®‡¶æ‡¶á ‡¶Ü‡¶á‡¶®‡¶™‡¶∏‡¶æ‡¶∏‡¶®‡¶ï‡ßá\n",
       "29997             ‡¶ï‡¶æ‡¶∏‡¶Æ‡¶ø‡¶∞ ‡¶ï‡¶æ‡¶∏‡¶Æ‡¶ø‡¶∞‡¶á ‡¶®‡¶ø‡¶ú‡¶∂‡ßç‡¶Ø‡¶á ‡¶∏‡¶æ‡¶¶‡¶ø‡¶® ‡¶π‡¶ì‡ßü‡¶æ‡¶∞ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞\n",
       "29998                   ‡¶ï‡¶≤‡¶Æ‡¶ø ‡¶™‡¶ø‡¶≤‡¶ø‡¶ú ‡¶Ü‡¶™‡ßÅ ‡¶Æ‡¶®‡¶ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ï‡¶ø‡¶ì‡¶ü ‡¶≤‡¶æ‡¶ó‡¶õ‡ßá\n",
       "29999                           ‡¶Ü‡¶Æ‡¶ø ‡¶™‡¶æ‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶® ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ú‡ßú‡¶ø‡¶§\n",
       "Name: sentence, Length: 30000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors = pd.DataFrame()\n",
    "\n",
    "for doc in df['sentence']:\n",
    "    temp = pd.DataFrame()\n",
    "    for word_a in doc.split():\n",
    "        try:\n",
    "            word_vec=model.wv[word_a]\n",
    "            temp=temp.append(pd.Series(word_vec),ignore_index=True)  \n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean()\n",
    "    docs_vectors=docs_vectors.append(doc_vector,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors['hate'] = df['hate']\n",
    "docs_vectors = docs_vectors.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset in Test & Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23962, 300), (23962,), (5991, 300), (5991,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('hate', axis = 1),\n",
    "                                                   docs_vectors['hate'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression To Classify the Embaddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.694041\n",
      "Precision: 0.663430\n",
      "Recall: 0.201474\n",
      "F1 score: 0.309084\n",
      "ROC AUC: 0.574448\n"
     ]
    }
   ],
   "source": [
    "test_pred = logisticRegr.predict(test_x)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "accuracy=accuracy_score(test_y, test_pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(test_y, test_pred)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(test_y, test_pred)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(test_y, test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(test_y, test_pred)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM to classify the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.701052\n",
      "Precision: 0.659686\n",
      "Recall: 0.247666\n",
      "F1 score: 0.360129\n",
      "ROC AUC: 0.590971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC() # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "test_pred = clf.predict(test_x)\n",
    "accuracy=accuracy_score(test_y, test_pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(test_y, test_pred)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(test_y, test_pred)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(test_y, test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(test_y, test_pred)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTM Model To build the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index={}\n",
    "f=open(os.path.join('','embedding_word2vec.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    value=line.split()\n",
    "    word=value[0]\n",
    "    coefs=np.asarray(value[1:])\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOund 57362 unique tokens.\n",
      "shape of review tensor: (30000, 537)\n",
      "shape of sentiment tensor: (30000,)\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in df['sentence']])\n",
    "#max_length=537\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(df['sentence'])\n",
    "sequences = tokenizer_obj.texts_to_sequences(df['sentence'])\n",
    "\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('FOund %s unique tokens.'%len(word_index))\n",
    "\n",
    "review_pad=pad_sequences(sequences,maxlen=max_length)\n",
    "sentiment=df['hate'].values\n",
    "print('shape of review tensor:',review_pad.shape)\n",
    "print('shape of sentiment tensor:', sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in df['sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59062"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]= embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57363\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 537)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57363, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "modell=Sequential()\n",
    "embedding_layer = Embedding(num_words, 300, embeddings_initializer=Constant(embedding_matrix),input_length=max_length,trainable=False)\n",
    "modell.add(embedding_layer)\n",
    "modell.add(LSTM(units=32,dropout=0.2,recurrent_dropout=0.2))\n",
    "modell.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 537, 300)          17208900  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 17,251,557\n",
      "Trainable params: 42,657\n",
      "Non-trainable params: 17,208,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modell.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT=0.2\n",
    "indices=np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "sentiment=sentiment[indices]\n",
    "num_validation_samples=int(VALIDATION_SPLIT*review_pad.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainn_pad=review_pad[:-num_validation_samples]\n",
    "y_trainn=sentiment[:-num_validation_samples]\n",
    "x_testt_pad=review_pad[-num_validation_samples:]\n",
    "y_testt=sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 537)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_testt_pad.shape)\n",
    "print(y_testt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "188/188 [==============================] - 330s 2s/step - loss: 0.6525 - accuracy: 0.6592\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 291s 2s/step - loss: 0.6435 - accuracy: 0.6578\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 281s 1s/step - loss: 0.6358 - accuracy: 0.6682\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 323s 2s/step - loss: 0.6381 - accuracy: 0.6649\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 305s 2s/step - loss: 0.6349 - accuracy: 0.6695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fd6d933d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modell.fit(x_trainn_pad,y_trainn,batch_size=128,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = modell.predict(x_testt_pad)\n",
    "# accuracy=accuracy_score(y_testt, test_pred)\n",
    "# print('Accuracy: %f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_testt, test_pred)\n",
    "# print('Precision: %f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_testt, test_pred)\n",
    "# print('Recall: %f' % recall)\n",
    "# # f1: 2 tp / (2 tp + fp + fn)\n",
    "# f1 = f1_score(y_testt, test_pred)\n",
    "# print('F1 score: %f' % f1)\n",
    "# # ROC AUC\n",
    "# auc = roc_auc_score(y_testt, test_pred)\n",
    "# print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.671500\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_testt, test_pred.round())\n",
    "print('Accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "test_pred_bool = np.argmax(test_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_testt, test_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_testt, test_pred , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.509493\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_testt, test_pred)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
